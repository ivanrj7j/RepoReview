{'readme': '# GAN-Based-Super-resolution\nImplementation of Super resolution models using GANs\n\n## Goals \n\n- Train a model that can upscale any given image to 4x size\n- Implement the [SRGAN](https://arxiv.org/pdf/1609.04802) paper\n- Feed the depth data to the model for better understanding. Using [MiDaS](https://pytorch.org/hub/intelisl_midas_v2/)\n- Only use MSE for comparing\n    -  Very bad results, worse that simple bicubic expansion\n- Use a hybrid mse first training then, use GAN\n- Compare the difference between using GAN, hybrid approach and MSE only.\n- Compare results with or without Depth\n- Experiment with output of range [0, 1] instead of [-1, 1]\n- Instead of using a classifier inside the discriminator itself, try just going with the nxn representation instead that way you can have loss for each region like in pix2pix networks.\n\n`README.md will be updated based on the results of the project later`\n', 'description': 'Implementation of Super resolution models using GANs', 'contents': ['from train.vggTrain import train\n\nif __name__ == "__main__":\n    train()', 'import torch.nn as nn\nfrom src.blocks import Upscale2xBlock, ResBlock, MiDaSBlock\nimport torch\nfrom math import log2\n\nclass Generator(nn.Module):\n    def __init__(self, midas:nn.Module=None, imageChannels:int=3, scale:int=4, *args, **kwargs) -> None:\n        """\n        Initializes the generator\n\n        Parmeters:\n        midas (nn.Module): The MiDaS model\n        imageChannels (int): Number of channels in the input image (default 3)\n        scale (int): Scale factor for the output image, it should be a power of 2 (default 4).\n\n        Example:\n        >>> import torch\n        >>> midasType = "MiDaS_small"\n        >>> midas = torch.hub.load("intel-isl/MiDaS", midasType).to("cuda")\n        >>> generator = Generator(midas)\n        """\n        super().__init__(*args, **kwargs)\n        self.midas = midas\n        inChannels = imageChannels\n\n\n        if midas is not None:\n            self.midas_block = MiDaSBlock(midas)\n            inChannels += 1\n\n        self.initBlock = nn.Sequential(\n            nn.Conv2d(inChannels, 64, kernel_size=9, stride=1, padding="same"),\n            nn.PReLU()\n        )\n\n        residualBlocks = [ResBlock(64, 64, kernelSize=3, stride=1) for _ in range(17)]\n        finalResidual = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding="same"),\n            nn.BatchNorm2d(64)\n        )\n        residualBlocks.append(finalResidual)\n        self.residualBlocks = nn.Sequential(*residualBlocks)\n\n        factor = int(log2(scale))\n        self.upscaleBlocks = nn.Sequential(*(Upscale2xBlock(64, 256) for _ in range(factor)))\n\n        self.finalLayer = nn.Conv2d(64, imageChannels, kernel_size=9, stride=1, padding="same")\n\n    def forward(self, x):\n        if self.midas is not None:\n            depthMap = self.midas_block(x)\n            x = torch.concat((x, depthMap), 1)\n
# getting depth info \n            \n        initOutput = self.initBlock.forward(x)\n        # passing through the first block \n        \n        residualOutput = self.residualBlocks.forward(initOutput)\n        residualOutput += initOutput\n        # passing through the residual blocks \n\n        upscaleOutput = self.upscaleBlocks.forward(residualOutput)\n        # upscaling the image \n        \n        finalOutput = self.finalLayer.forward(upscaleOutput)\n\n        return torch.tanh(finalOutput)\n        # applying tanh to the final output is not in the paper ', 'import torch.nn as nn\nfrom src.blocks import DiscriminatorBlock\n\nclass Discriminator(nn.Module):\n    def __init__(self, imageChannels:int=3, useClassifier:bool=True, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.useClassifier = useClassifier\n\n        block64 = nn.Sequential(\n            DiscriminatorBlock(imageChannels, 64, kernelSize=3, stride=1, useBatchNorm=False),\n            DiscriminatorBlock(64, 64, kernelSize=3, stride=2)\n        )\n        block128 = nn.Sequential(\n            DiscriminatorBlock(64, 128, kernelSize=3, stride=1),\n            DiscriminatorBlock(128, 128, kernelSize=3, stride=2)\n        )\n        block256 = nn.Sequential(\n            DiscriminatorBlock(128, 256, kernelSize=3, stride=1),\n            DiscriminatorBlock(256, 256, kernelSize=3, stride=2)\n        )\n        block512 = nn.Sequential(\n            DiscriminatorBlock(256, 512, kernelSize=3, stride=1),\n            DiscriminatorBlock(512, 512, kernelSize=3, stride=2)\n        )\n\n        self.convBlocks = nn.Sequential(block64, block128, block256, block512)\n        \n        if useClassifier:\n            self.final = nn.Sequential(\n                nn.AdaptiveAvgPool2d((6, 6)),\n                nn.Flatten(),\n                nn.Linear(512 * 6 * 6, 1024),\n                nn.LeakyReLU(0.2, True),\n                nn.Linear(1024, 1)\n            )\n        else:\n            self.final = DiscriminatorBlock(512, 1, kernelSize=3, stride=2)\n\n    def forward(self, x):\n        convResults = self.convBlocks.forward(x)\n\n        return self.final.forward(convResults)        ', 'import torch.nn as nn\nfrom utils import loadMidas, loadModels\nimport config\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom utils import getDataloader, writeSummary, saveModels\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport torch\nfrom src import VGGLoss\n\ngenerator, discriminator = loadModels(config.generatorPath, config.discriminatorPath, None, 3, config.scale, config.device)\n\nif config.shouldLoadMidas:\n    midas = loadMidas()\n    generator, discriminator = loadModels(config.generatorPath, config.discriminatorPath, midas, 3, config.scale, config.device)\n\ngenOpt = Adam(generator.parameters(), config.lr)\ndiscOpt = Adam(discriminator.parameters(), config.lr)\n\ntrainLoader = getDataloader(config.trainDatasetPath, config.scale, config.resolution, config.assumeResolution, config.numWorkers, config.batchSize)\nvalLoader = getDataloader(config.valDatasetPath, config.scale, config.resolution, config.assumeResolution, config.numWorkers, config.batchSize)\n\nwriter = SummaryWriter(f"runs/vgg/{round(time.time())}")\n\nbce = nn.BCEWithLogitsLoss()\nvggLossFunc = VGGLoss()\n\ngenScheduler = ExponentialLR(genOpt, config.decayGamma)\ndiscScheduler = ExponentialLR(discOpt, config.decayGamma)\n\ndef trainStep(x, y):\n    x, y = x.to(config.device), y.to(config.device)\n            \n    \n    generated = generator(x)\n\n    discReal = discriminator.forward(y)\n    discFake = discriminator.forward(generated.detach())\n        \n    discRealLoss = bce.forward(discReal, torch.ones_like(discReal)-(torch.rand_like(discReal) * 0.08))\n    discFakeLoss = bce.forward(discFake, torch.zeros_like(discFake)+(torch.rand_like(discFake) * 0.08))\n    discLoss = discFakeLoss + discRealLoss\n            \n    discOpt.zero_grad()\n    discLoss.backward()\n    discOpt.step()\n\n    discFake = discriminator.forward(generated)\n    adveserialLoss = config.adLambda * bce.forward(discFake, torch.ones_like(discFake))\n    vggLoss = config.vggLambda * vggLossFunc.forward(generated, y)\n    genLoss = adveserialLoss + vggLoss\n\n    genOpt.zero_grad()\n    genLoss.backward()\n    genOpt.step()\n\n    return genLoss.item(), discLoss.item()\n\n\ndef train():\n    print("Training with VGG Loss...")\n    for epoch in range(1, config.epochs+1):\n        start = time.time()\n\n        generator.train()\n        discriminator.train()\n\n        genLoss = 0\n        discLoss = 0\n\n        loop = tqdm(trainLoader, f"[{epoch}/{config.epochs}]", len(trainLoader), leave=False, unit="batch")\n\n        for i, (x, y) in enumerate(loop, start=1):\n            currentGenLoss, currentDiscLoss = trainStep(x, y)\n
  genLoss += currentGenLoss\n            discLoss += currentDiscLoss\n            loop.set_postfix(genLoss=genLoss/i, discLoss=discLoss/i)\n        \n        genLoss = genLoss / len(trainLoader)\n        discLoss = discLoss / len(trainLoader)\n\n        if epoch % config.decayEvery == 0:\n            genScheduler.step()\n            discScheduler.step()\n            print(f"Decaying [{genOpt.param_groups[0]["lr"]}]")\n\n        generator.eval()\n        discriminator.eval()\n\n        for x, y in valLoader:\n            x, y = x.to(config.device), y.to(config.device)\n            writeSummary(writer, x, y, generator, {"genLoss": genLoss, "discLoss":discLoss}, epoch)\n            break\n\n        if epoch % config.checkpointInterval == 0:\n            saveModels(generator, discriminator, config.savePath, f"vgg-{epoch}")\n        \n        elapsed = round(time.time() - start)\n        print(f"[{epoch}/{config.epochs} ] Gen Loss: {float(genLoss):.2f} Dis Loss: {float(discLoss):.2f} ({elapsed}s)")\n\n    saveModels(generator, None, config.savePath, "vgg-final")\n    writer.close()\n    print("Training completed.")', '# GAN-Based-Super-resolution\nImplementation of Super resolution models using GANs\n\n## Goals \n\n- Train a model that can upscale any given image to 4x size\n- Implement the [SRGAN](https://arxiv.org/pdf/1609.04802) paper\n- Feed the depth data to the model for better understanding. Using [MiDaS](https://pytorch.org/hub/intelisl_midas_v2/)\n- Only use MSE for comparing\n    -  Very bad results, worse that simple bicubic expansion\n- Use a hybrid mse first training then, use GAN\n- Compare the difference between using GAN, hybrid approach and MSE only.\n- Compare results with or without Depth\n- Experiment with output of range [0, 1] instead of [-1, 1]\n- Instead of using a classifier inside the discriminator itself, try just going with the nxn representation instead that way you can have loss for each region like in pix2pix networks.\n\n`README.md will be updated based on the results of the project later`\n'], 'structure': {'.gitignore': 'file', 'LICENSE': 'file', 'README.md': 'file', 'config.py': 'file', 'main.py': 'file', 'src': {'__init__.py': 'file', 'blocks.py': 'file', 'discriminator.py': 'file', 'generator.py': 'file', 'loss.py': 'file'}, 'train': {'mseTrain.py': 'file', 'vggTrain.py': 'file'}, 'utils': {'__init__.py': 'file', 'dataloader.py': 'file', 'utils.py': 'file'}}}